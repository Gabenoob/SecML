{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data into train and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converlutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('train-minst.csv')\n",
    "# data = pd.read_csv('test-minst.csv') # only for architecture building\n",
    "label = data['label']\n",
    "data = data.values[:, 1:]\n",
    "data = data.reshape(data.shape[0], 28, 28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 1000\n",
    "plt.imshow(data[n], cmap='gray')\n",
    "print(label[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## super parameter (n_layer, stoppinh criteria, con_kernal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = torch.nn.Linear(32*7*7, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.conv1(x))\n",
    "        x = torch.nn.functional.max_pool2d(x, 2)\n",
    "        x = torch.nn.functional.relu(self.conv2(x))\n",
    "        x = torch.nn.functional.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 32*7*7)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 12.782937049865723\n",
      "Epoch: 1, Loss: 13.440282821655273\n",
      "Epoch: 2, Loss: 16.70424461364746\n",
      "Epoch: 3, Loss: 12.112531661987305\n",
      "Epoch: 4, Loss: 9.047890663146973\n",
      "Epoch: 5, Loss: 5.775685787200928\n",
      "Epoch: 6, Loss: 4.835099220275879\n",
      "Epoch: 7, Loss: 4.720396518707275\n",
      "Epoch: 8, Loss: 4.398662090301514\n",
      "Epoch: 9, Loss: 3.693903923034668\n",
      "Epoch: 10, Loss: 2.888331174850464\n",
      "Epoch: 11, Loss: 2.266852617263794\n",
      "Epoch: 12, Loss: 1.8493179082870483\n",
      "Epoch: 13, Loss: 1.5936979055404663\n",
      "Epoch: 14, Loss: 1.4534045457839966\n",
      "Epoch: 15, Loss: 1.3755587339401245\n",
      "Epoch: 16, Loss: 1.3122528791427612\n",
      "Epoch: 17, Loss: 1.2432700395584106\n",
      "Epoch: 18, Loss: 1.168607234954834\n",
      "Epoch: 19, Loss: 1.0960726737976074\n",
      "Epoch: 20, Loss: 1.0278412103652954\n",
      "Epoch: 21, Loss: 0.9670742154121399\n",
      "Epoch: 22, Loss: 0.915435254573822\n",
      "Epoch: 23, Loss: 0.871012270450592\n",
      "Epoch: 24, Loss: 0.8298389911651611\n",
      "Epoch: 25, Loss: 0.7890974879264832\n",
      "Epoch: 26, Loss: 0.7477770447731018\n",
      "Epoch: 27, Loss: 0.7048733234405518\n",
      "Epoch: 28, Loss: 0.6603326797485352\n",
      "Epoch: 29, Loss: 0.615854799747467\n",
      "Epoch: 30, Loss: 0.5736463665962219\n",
      "Epoch: 31, Loss: 0.533105194568634\n",
      "Epoch: 32, Loss: 0.4928896129131317\n",
      "Epoch: 33, Loss: 0.4553341567516327\n",
      "Epoch: 34, Loss: 0.4247570335865021\n",
      "Epoch: 35, Loss: 0.40116265416145325\n",
      "Epoch: 36, Loss: 0.3818475604057312\n",
      "Epoch: 37, Loss: 0.36556360125541687\n",
      "Epoch: 38, Loss: 0.3522767722606659\n",
      "Epoch: 39, Loss: 0.3411101698875427\n",
      "Epoch: 40, Loss: 0.3298158347606659\n",
      "Epoch: 41, Loss: 0.31709206104278564\n",
      "Epoch: 42, Loss: 0.30381718277931213\n",
      "Epoch: 43, Loss: 0.29139572381973267\n",
      "Epoch: 44, Loss: 0.2806302011013031\n",
      "Epoch: 45, Loss: 0.2713657319545746\n",
      "Epoch: 46, Loss: 0.2632945477962494\n",
      "Epoch: 47, Loss: 0.25633004307746887\n",
      "Epoch: 48, Loss: 0.2502405345439911\n",
      "Epoch: 49, Loss: 0.24450796842575073\n",
      "Epoch: 50, Loss: 0.2386646866798401\n",
      "Epoch: 51, Loss: 0.23280680179595947\n",
      "Epoch: 52, Loss: 0.22734396159648895\n",
      "Epoch: 53, Loss: 0.2224562168121338\n",
      "Epoch: 54, Loss: 0.21783821284770966\n",
      "Epoch: 55, Loss: 0.2130957692861557\n",
      "Epoch: 56, Loss: 0.2080790251493454\n",
      "Epoch: 57, Loss: 0.2029702216386795\n",
      "Epoch: 58, Loss: 0.1980530321598053\n",
      "Epoch: 59, Loss: 0.19345474243164062\n",
      "Epoch: 60, Loss: 0.18917828798294067\n",
      "Epoch: 61, Loss: 0.18524466454982758\n",
      "Epoch: 62, Loss: 0.18164271116256714\n",
      "Epoch: 63, Loss: 0.17828533053398132\n",
      "Epoch: 64, Loss: 0.17498718202114105\n",
      "Epoch: 65, Loss: 0.17163245379924774\n",
      "Epoch: 66, Loss: 0.16827327013015747\n",
      "Epoch: 67, Loss: 0.16500987112522125\n",
      "Epoch: 68, Loss: 0.16187137365341187\n",
      "Epoch: 69, Loss: 0.15881314873695374\n",
      "Epoch: 70, Loss: 0.15582551062107086\n",
      "Epoch: 71, Loss: 0.15294256806373596\n",
      "Epoch: 72, Loss: 0.15019044280052185\n",
      "Epoch: 73, Loss: 0.1475612372159958\n",
      "Epoch: 74, Loss: 0.14503245055675507\n",
      "Epoch: 75, Loss: 0.14260919392108917\n",
      "Epoch: 76, Loss: 0.1402871310710907\n",
      "Epoch: 77, Loss: 0.13803939521312714\n",
      "Epoch: 78, Loss: 0.13583165407180786\n",
      "Epoch: 79, Loss: 0.1336517333984375\n",
      "Epoch: 80, Loss: 0.13152645528316498\n",
      "Epoch: 81, Loss: 0.12948833405971527\n",
      "Epoch: 82, Loss: 0.12751558423042297\n",
      "Epoch: 83, Loss: 0.12559299170970917\n",
      "Epoch: 84, Loss: 0.12371496111154556\n",
      "Epoch: 85, Loss: 0.12189154326915741\n",
      "Epoch: 86, Loss: 0.12012992054224014\n",
      "Epoch: 87, Loss: 0.11840885132551193\n",
      "Epoch: 88, Loss: 0.11674188822507858\n",
      "Epoch: 89, Loss: 0.11512741446495056\n",
      "Epoch: 90, Loss: 0.11354620009660721\n",
      "Epoch: 91, Loss: 0.11199179291725159\n",
      "Epoch: 92, Loss: 0.11047054082155228\n",
      "Epoch: 93, Loss: 0.10899661481380463\n",
      "Epoch: 94, Loss: 0.10756704211235046\n",
      "Epoch: 95, Loss: 0.10616698116064072\n",
      "Epoch: 96, Loss: 0.10479072481393814\n",
      "Epoch: 97, Loss: 0.1034427285194397\n",
      "Epoch: 98, Loss: 0.10212550312280655\n",
      "Epoch: 99, Loss: 0.100844606757164\n",
      "Epoch: 100, Loss: 0.09960078448057175\n",
      "Epoch: 101, Loss: 0.09839236736297607\n",
      "Epoch: 102, Loss: 0.09721818566322327\n",
      "Epoch: 103, Loss: 0.09606397897005081\n",
      "Epoch: 104, Loss: 0.09493326395750046\n",
      "Epoch: 105, Loss: 0.09383514523506165\n",
      "Epoch: 106, Loss: 0.09275862574577332\n",
      "Epoch: 107, Loss: 0.09170443564653397\n",
      "Epoch: 108, Loss: 0.09067796170711517\n",
      "Epoch: 109, Loss: 0.08967147022485733\n",
      "Epoch: 110, Loss: 0.08869051933288574\n",
      "Epoch: 111, Loss: 0.0877329632639885\n",
      "Epoch: 112, Loss: 0.08679477870464325\n",
      "Epoch: 113, Loss: 0.08587954938411713\n",
      "Epoch: 114, Loss: 0.08498187363147736\n",
      "Epoch: 115, Loss: 0.0841042622923851\n",
      "Epoch: 116, Loss: 0.08324039727449417\n",
      "Epoch: 117, Loss: 0.08239702135324478\n",
      "Epoch: 118, Loss: 0.08156835287809372\n",
      "Epoch: 119, Loss: 0.08075816184282303\n",
      "Epoch: 120, Loss: 0.07996547967195511\n",
      "Epoch: 121, Loss: 0.07918380200862885\n",
      "Epoch: 122, Loss: 0.07841651141643524\n",
      "Epoch: 123, Loss: 0.07766447961330414\n",
      "Epoch: 124, Loss: 0.07692331820726395\n",
      "Epoch: 125, Loss: 0.0761994943022728\n",
      "Epoch: 126, Loss: 0.07548702508211136\n",
      "Epoch: 127, Loss: 0.07478439062833786\n",
      "Epoch: 128, Loss: 0.0740971714258194\n",
      "Epoch: 129, Loss: 0.07341945171356201\n",
      "Epoch: 130, Loss: 0.07275241613388062\n",
      "Epoch: 131, Loss: 0.0720972791314125\n",
      "Epoch: 132, Loss: 0.07145031541585922\n",
      "Epoch: 133, Loss: 0.07081317156553268\n",
      "Epoch: 134, Loss: 0.07018381357192993\n",
      "Epoch: 135, Loss: 0.06956921517848969\n",
      "Epoch: 136, Loss: 0.0689617395401001\n",
      "Epoch: 137, Loss: 0.06836544722318649\n",
      "Epoch: 138, Loss: 0.06777776777744293\n",
      "Epoch: 139, Loss: 0.06719958782196045\n",
      "Epoch: 140, Loss: 0.06663177162408829\n",
      "Epoch: 141, Loss: 0.06607190519571304\n",
      "Epoch: 142, Loss: 0.06551916152238846\n",
      "Epoch: 143, Loss: 0.06497640907764435\n",
      "Epoch: 144, Loss: 0.06444153189659119\n",
      "Epoch: 145, Loss: 0.06391293555498123\n",
      "Epoch: 146, Loss: 0.06339432299137115\n",
      "Epoch: 147, Loss: 0.06287792325019836\n",
      "Epoch: 148, Loss: 0.062371332198381424\n",
      "Epoch: 149, Loss: 0.06186951324343681\n",
      "Epoch: 150, Loss: 0.061374690383672714\n",
      "Epoch: 151, Loss: 0.06088770553469658\n",
      "Epoch: 152, Loss: 0.06040804460644722\n",
      "Epoch: 153, Loss: 0.05993286147713661\n",
      "Epoch: 154, Loss: 0.0594656877219677\n",
      "Epoch: 155, Loss: 0.05900510400533676\n",
      "Epoch: 156, Loss: 0.05854865536093712\n",
      "Epoch: 157, Loss: 0.058098915964365005\n",
      "Epoch: 158, Loss: 0.05765771493315697\n",
      "Epoch: 159, Loss: 0.057220280170440674\n",
      "Epoch: 160, Loss: 0.0567881315946579\n",
      "Epoch: 161, Loss: 0.05636214092373848\n",
      "Epoch: 162, Loss: 0.05594184249639511\n",
      "Epoch: 163, Loss: 0.05552515387535095\n",
      "Epoch: 164, Loss: 0.055114395916461945\n",
      "Epoch: 165, Loss: 0.05470893159508705\n",
      "Epoch: 166, Loss: 0.05430714786052704\n",
      "Epoch: 167, Loss: 0.053914107382297516\n",
      "Epoch: 168, Loss: 0.0535212978720665\n",
      "Epoch: 169, Loss: 0.05313386395573616\n",
      "Epoch: 170, Loss: 0.0527520477771759\n",
      "Epoch: 171, Loss: 0.05237274616956711\n",
      "Epoch: 172, Loss: 0.05199771374464035\n",
      "Epoch: 173, Loss: 0.05162623152136803\n",
      "Epoch: 174, Loss: 0.05125931650400162\n",
      "Epoch: 175, Loss: 0.05089540407061577\n",
      "Epoch: 176, Loss: 0.05053507909178734\n",
      "Epoch: 177, Loss: 0.050179433077573776\n",
      "Epoch: 178, Loss: 0.04982763156294823\n",
      "Epoch: 179, Loss: 0.04947684705257416\n",
      "Epoch: 180, Loss: 0.049131207168102264\n",
      "Epoch: 181, Loss: 0.0487884059548378\n",
      "Epoch: 182, Loss: 0.048446763306856155\n",
      "Epoch: 183, Loss: 0.04811003431677818\n",
      "Epoch: 184, Loss: 0.047775860875844955\n",
      "Epoch: 185, Loss: 0.04744389280676842\n",
      "Epoch: 186, Loss: 0.04711693897843361\n",
      "Epoch: 187, Loss: 0.046790044754743576\n",
      "Epoch: 188, Loss: 0.046467963606119156\n",
      "Epoch: 189, Loss: 0.046147674322128296\n",
      "Epoch: 190, Loss: 0.045829109847545624\n",
      "Epoch: 191, Loss: 0.0455162338912487\n",
      "Epoch: 192, Loss: 0.04520539566874504\n",
      "Epoch: 193, Loss: 0.04489538446068764\n",
      "Epoch: 194, Loss: 0.04459083825349808\n",
      "Epoch: 195, Loss: 0.044287942349910736\n",
      "Epoch: 196, Loss: 0.04398869723081589\n",
      "Epoch: 197, Loss: 0.0436902679502964\n",
      "Epoch: 198, Loss: 0.043395332992076874\n",
      "Epoch: 199, Loss: 0.043102528899908066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19205/3776535719.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  Net.load_state_dict(torch.load('model.pth'))\n"
     ]
    }
   ],
   "source": [
    "Net = CNN()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(Net.parameters(), lr=0.001)\n",
    "\n",
    "data = torch.tensor(data, dtype=torch.float32).unsqueeze(1)\n",
    "label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# move the model to GPU\n",
    "Net = Net.cuda()\n",
    "criterion = criterion.cuda()\n",
    "data = data.cuda()\n",
    "label = label.cuda()\n",
    "\n",
    "\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    output = Net(data)\n",
    "    loss = criterion(output, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('Epoch: {}, Loss: {}'.format(epoch, loss.item()))\n",
    "\n",
    "# Save the model\n",
    "torch.save(Net.state_dict(), 'model.pth')\n",
    "\n",
    "# Load the model\n",
    "Net = CNN()\n",
    "Net.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "# Test the model\n",
    "data = pd.read_csv('train-minst.csv')\n",
    "data = data.values[:, 1:]\n",
    "data = data.reshape(data.shape[0], 28, 28)\n",
    "data = torch.tensor(data, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "output = Net(data)\n",
    "output = torch.argmax(output, dim=1)\n",
    "output = output.numpy()\n",
    "\n",
    "df = pd.DataFrame(output, columns=['label'])\n",
    "df.index.name = 'id'\n",
    "df.to_csv('output.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train-minst.csv')\n",
    "data = data.values[:, 0]\n",
    "# 合并两个数组\n",
    "data = np.vstack((data, output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('comp.csv', 'w') as f:\n",
    "    f.write('true,pred\\n')\n",
    "    for i in range(data.shape[1]):\n",
    "        f.write('{},{}\\n'.format(data[0, i], data[1, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9873095238095239\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy\n",
    "with open('comp.csv', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for line in lines[1:]:\n",
    "        true, pred = line.strip().split(',')\n",
    "        if true == pred:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    print('Accuracy: {}'.format(correct/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## speed of convergerce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BSSMF\n",
    "\n",
    "### 分解后得10列矩阵，由1～0全部拆开，1全部相加取平均，2同理，10个数分别做一个预测，差距最小的为预测值。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
